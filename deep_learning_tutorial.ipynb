{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVjl-kju6o4S"
   },
   "source": [
    "# Deep learning tutorial: particle identification\n",
    "\n",
    "Welcome to this tutorial on deep learning for particle identification in neutrino events! In this tutorial, you will learn how to define and run deep-learning methods to identify particles in neutrino events.\n",
    "\n",
    "Deep learning is a powerful subset of machine learning that involves training complex artificial neural networks to solve problems that traditional machine-learning techniques may struggle with. In this tutorial, we will dive deeper into the world of deep learning and explore how to train a neural network to identify particles in neutrino events. By the end of this tutorial, you will have a solid understanding of deep learning concepts and how they can be applied in a practical setting. So let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG5VHLK7-iqp"
   },
   "source": [
    "##Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaUCNuA9DBG5"
   },
   "source": [
    "To start off, let's check if a GPU is available and turn it on if it is. Using a GPU can significantly speed up the training process of our neural network, so it's always a good idea to take advantage of it if possible:\n",
    "\n",
    "```\n",
    "Edit -> Notebook settings -> Hardware accelerator: GPU -> Save.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHMkB-apwSJy"
   },
   "source": [
    "Before we can begin training our neural network, we need to download the dataset and import the necessary Python packages and modules.\n",
    "\n",
    "Downloading the dataset will provide us with the data we need to train and test our neural network. Additionally, we'll need to import various packages and modules such as NumPy, Pandas, Matplotlib, and TensorFlow to handle the data, visualize it, and define and train our neural network.\n",
    "\n",
    "Let's get started by downloading the dataset and importing the required packages and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeZ5YQbaDkQO",
    "outputId": "3f717198-579d-4d2f-952a-a08945005aa0"
   },
   "outputs": [],
   "source": [
    "!wget \"https://cernbox.cern.ch/remote.php/dav/public-files/BDuycrUjVffTCxJ/modules.py\"\n",
    "!wget \"https://cernbox.cern.ch/remote.php/dav/public-files/IAxTMwkrAhFKFGz/df_tutorial.p\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXsuG5BQK8wF"
   },
   "source": [
    "Before we proceed, let's check if the GPU was successfully detected. Using a GPU can significantly accelerate the training process of our neural network, so it's important to ensure that it's working properly.\n",
    "\n",
    "Let's run a quick check to confirm if the GPU is available and ready to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GE94a0PDR1N",
    "outputId": "1d56c023-51de-4425-dcba-4f3c3a2c1332"
   },
   "outputs": [],
   "source": [
    "# Check if a GPU is available and turn it on if possible\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    # Configure the GPU device\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name != '/device:GPU:0':\n",
    "        raise SystemError('GPU device not found')\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVHwIwSOvz3n"
   },
   "source": [
    "##Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc7lPCGT6o4W"
   },
   "source": [
    "Now that we have downloaded the dataset, we can proceed to load it into our notebook. This dataset contains the necessary information for training and testing our neural network.\n",
    "\n",
    "Let's load the dataset into our notebook so that we can start preparing it for use in our deep-learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEgXr0CV6o4W"
   },
   "outputs": [],
   "source": [
    "# Read dataframe\n",
    "df = pd.read_pickle('df_tutorial.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSBVlOV-6o4W"
   },
   "source": [
    "Let's take a look at the dataset we'll be working with. The dataset consists of 59,578 particle gun events from the [SFGD detector of the T2K experiment](https://doi.org/10.1088/1748-0221/15/12/p12003), each with the following attributes:\n",
    "\n",
    "- **TruePID**: PDG code for particle identification (PID); 2212 (proton), 13 (muon), 211 (pion).\n",
    "- **TrueMomentum**: momentum in MeV/c.\n",
    "- **NNodes**: number of nodes of the event (3D spatial points).\n",
    "- **NodeOrder**: order of the nodes within the event.\n",
    "- **NodePosX**: array with the coordinates of the nodes along the X-axis (in mm).\n",
    "- **NodePosY**: array with the coordinates of the nodes along the Y-axis (in mm).\n",
    "- **NodePosZ**: array with the coordinates of the nodes along the Z-axis (in mm).\n",
    "- **NodeT**: array with the timestamps of the nodes (in ns).\n",
    "- **Nodededx**: array with energy deposits of the nodes (dE/dx).\n",
    "- **TrkLen**: length of the track (in mm).\n",
    "- **TrkEDepo**: total track energy deposition (in arbitrary units).\n",
    "- **TrkDir1**: track direction, polar angle (theta, in radians).\n",
    "- **TrkDir2**: track direction, azimuth angle (phi, in radians).\n",
    "\n",
    "These attributes will be used to train a neural network to identify particles in neutrino events. Let's take a first look at the dataset by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "id": "o9FuASKM6o4X",
    "outputId": "c6bebf91-3271-45bd-98a0-73034717d7c8"
   },
   "outputs": [],
   "source": [
    "# Print the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RghPKyXA04rQ"
   },
   "source": [
    "We can also check the correlations among the variables, with the exception of the node features, since each event has a different number of nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "Ag5QkbJKMsVm",
    "outputId": "d8d14870-9467-4bdd-f918-30f370325d68"
   },
   "outputs": [],
   "source": [
    "# Print variable correlations\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrzNwzscqd4w"
   },
   "source": [
    "The 3D spatial points of the events are typically represented as nodes, which correspond to fitted positions after track reconstruction. To visualize the events, we can plot the nodes in the detector space. By default, the code displays the nodes for the first event (event 0), but we can visualize nodes for other events by adjusting the event_number variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "8c9eBj3T6o4Y",
    "outputId": "2d7eb8e6-f380-432c-896a-67de98896082"
   },
   "outputs": [],
   "source": [
    "# Set the event number to plot\n",
    "event_number = 0\n",
    "\n",
    "# Call the plot_event function to display the nodes in the detector space for the chosen event\n",
    "plot_event(df, event_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g70lBcA36o4Z"
   },
   "source": [
    "Preprocessing the data is essential regardless of the data type and algorithm chosen. The goal is to prepare the data to make it understandable for the machine learning algorithm. In our case, we want to learn to predict a label **y** from a fixed-size vector of features **X**. However, the input data is in 3D, and every event (track) has a different size, making it unsuitable for most machine learning algorithms.\n",
    "\n",
    "To fix this, we need to extract a fixed number of features from each event. One approach is to use `TrkLen` and `TrkEDepo` as features. This approach reduces the dimensionality of the data and makes it more suitable for machine learning algorithms.\n",
    "\n",
    "In addition, we will encode the particle identification (PID) codes for protons (2212), muons (13), and pions (211) into 0, 1, and 2, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-yfqZYjY6o4Z",
    "outputId": "96f45f27-7e19-473f-e837-73cc8664a2f5"
   },
   "outputs": [],
   "source": [
    "# Create input X and labels y from the dataset\n",
    "X = np.zeros(shape=(len(df), 2), dtype=np.float32)\n",
    "y = np.zeros(shape=(len(df),), dtype=np.int32)\n",
    "\n",
    "# Fill dataset\n",
    "for event_n, event in df.iterrows():\n",
    "    \n",
    "    # Encode the PID label\n",
    "    pid_label = event['TruePID']\n",
    "    if pid_label == 2212: \n",
    "        pid_label = 0  # proton\n",
    "    elif pid_label == 13: \n",
    "        pid_label = 1  # muon\n",
    "    else:\n",
    "        pid_label = 2  # pion\n",
    "    \n",
    "    # Retrieve the first node and assign it to X\n",
    "    X[event_n, 0] = event['TrkLen']\n",
    "    X[event_n, 1] = event['TrkEDepo']\n",
    "\n",
    "    # Assign the encoded PID label to y\n",
    "    y[event_n] = pid_label\n",
    "\n",
    "# Standardize the dataset (mean=0, std=1)\n",
    "X_stan = scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vlvQHUc6o4Z"
   },
   "source": [
    "Visualizing the training data is an important step in understanding the data before training the model. To simplify our analysis, we can start by comparing only protons and muons (ignoring pions for the moment). One way to achieve this is to create a scatter plot of one feature against the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "SDQfji9m6o4a",
    "outputId": "5ab18127-fbcb-4a3d-eaa7-dd011eb8f3b6"
   },
   "outputs": [],
   "source": [
    "# Define the parameters and labels to plot\n",
    "param_names = ['TrkLen', 'TrkEDepo']\n",
    "y_names = ['proton', 'muon']\n",
    "\n",
    "# Plot the data points for protons and muons\n",
    "plot_params_pid(X[y!=2], y[y!=2], param_names, y_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-INAKi_YAYW"
   },
   "source": [
    "Good! The scatter plot shows that protons and muons have distinct distributions and can be visually separated from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee4lzVE1yP-m"
   },
   "source": [
    "## Fully connected neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX5jfc3Q6o4b"
   },
   "source": [
    "Training a deep learning algorithm is a challenging task that requires a careful approach. Typically, the algorithm learns from a training dataset until it can make accurate predictions on new, unseen data. To test how well the algorithm generalizes to new data, we split the original dataset into two groups (although sometimes it's divided into three groups, for simplicity, we will be using a two-group approach in this example):\n",
    "\n",
    "- Training set: The model learns from this set only, and it should be the largest set.\n",
    "- Test set: We use this set to evaluate the model once it's fully trained.\n",
    "\n",
    "In this example, we'll allocate 60% of the data to the training set and 40% to the test set. To avoid introducing any bias during training, we'll shuffle the training examples before feeding them to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74sobEvb6o4c"
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stan[y!=2], y[y!=2], test_size=0.4, \n",
    "                                                    random_state=7, shuffle=True)\n",
    "\n",
    "# Print the number of examples in each set\n",
    "print(\"Number of training examples: \", len(X_train))\n",
    "print(\"Number of test examples: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZoa_hnEWmz2"
   },
   "source": [
    "This tutorial aims to explore deep learning methods, which is a subset of machine learning that involves artificial neural networks. In this tutorial, we will implement a fully connected neural network using the Keras interface from the TensorFlow deep-learning framework. Keras is an ideal API for neural-network prototyping, as it provides high-level abstractions that make it easy to define, train, and evaluate complex models.\n",
    "\n",
    "In the architecture of our neural network, each neuron computes the function $\\sigma(w x + b)$, where $w$ and $b$ are the input weight and bias of the neuron, respectively, and $\\sigma$ is the [activation function](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6). The goal of the activation function is to introduce non-linearity into the model, which allows the neural network to learn more complex relationships between the input data and the output.\n",
    "\n",
    "Since it's a binary classification task (proton or muon), **the output neuron has a sigmoid activation** function. The [sigmoid activation](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/) function is a commonly used function in binary classification tasks like the one we have here, where the output must be either 0 or 1. The function maps any input value to a value between 0 and 1, and it has a characteristic S-shaped curve. The output can be interpreted as a probability of belonging to the positive class, where values close to 0 represent high probability of belonging to the negative class and values close to 1 represent high probability of belonging to the positive class.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://cernbox.cern.ch/remote.php/dav/public-files/TCMgCAYrreRyNJv/dense_nn.png?scalingup=0&preview=1&a=1&c=284811006121607168%3A5d14ad7d&x=1920&y=1920\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "For the loss function, we will use the [binary cross-entropy loss](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) function. Binary cross-entropy loss is used for binary classification problems and it measures the difference between the predicted probability distribution and the true probability distribution. The true probability distribution has values of either 0 or 1, depending on the class label, and the predicted probability distribution is given by the output of the sigmoid activation function. The binary cross-entropy loss is defined as:\n",
    "\n",
    "$$\\mathrm{Binary\\ Cross-Entropy\\ Loss}=\\frac{1}{m}\\sum_{i=1}^m y_i\\log(\\hat{y_i}) + (1-y_i)\\log(1-\\hat{y_i})$$\n",
    "\n",
    "where $m$ is the number of samples in the dataset, $y_i$ is the true label of the $i$-th sample, $\\hat{y_i}$ is the predicted probability of the $i$-th sample belonging to the positive class (i.e., class 1), and $\\log$ denotes the natural logarithm. This loss function is commonly used in binary classification problems where we aim to minimize the difference between the predicted and true class labels.\n",
    "\n",
    "In this code cell, we create and compile the fully connected neural network model using the Keras interface of TensorFlow. We use the Adam optimizer, which is a variant of Stochastic Gradient Descent (SGD), to update the weights during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dswZ3SjYMOKx",
    "outputId": "a7b915af-a75d-4436-b698-bef0768522c7"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(7) # for reproducibility\n",
    "\n",
    "num_features = 2 # TrkLen, TrkEDepo\n",
    "num_classes = 1 # one output unit is enough since it's a binary classification problem\n",
    "\n",
    "# Create the fully connected neural network model\n",
    "input_layer = Input(shape=(num_features,)) # input layer\n",
    "hidden_layer_1 = Dense(10, activation='relu')(input_layer) # hidden layer 1\n",
    "hidden_layer_2 = Dense(10, activation='relu')(hidden_layer_1) # hidden layer 2\n",
    "output_layer = Dense(num_classes, activation='sigmoid')(hidden_layer_2) # output layer\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4xZRB1yhAhc"
   },
   "source": [
    "We will train the fully connected neural network model with the following hyperparameters: 10 epochs and a batch size of 128. Let's take a moment to define some important terms used in deep learning:\n",
    "\n",
    "* Batch: a batch is a set of $n$ input examples (also called mini-batch) processed independently and in parallel. During training, a batch results in only one update to the model, which consists of one forward pass (prediction) and one backward pass (adjusting the model weights).\n",
    "\n",
    "* Epoch: one epoch is one forward and backward pass of all the training examples. In other words, an epoch is one pass over the entire dataset, and it is used to separate training into distinct phases. For a dataset consisting of $m$ training examples and a batch size of $n$, it will take $\\lceil \\frac{m}{n} \\rceil$ iterations to complete one epoch.\n",
    "\n",
    "In our case, we have 25468 events for training, and we will use a batch size of 128. Therefore, we need 199 iterations to complete one epoch. After 10 epochs, the model will have seen the training data 10 times and made updates to its weights based on the training examples.\n",
    "\n",
    "Let's train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79TWw6evN6GU",
    "outputId": "f7aa622d-26ce-47d6-f764-3989409d6365"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rhw-cLdmyAFT"
   },
   "source": [
    "It's also common to calculate some metrics to evaluate our deep-learning method's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9etmLMv7co3X",
    "outputId": "37318de4-d348-4da6-a7f1-e2d889a44238"
   },
   "outputs": [],
   "source": [
    "# Predict classes on test set using the trained model\n",
    "y_pred = model.predict(X_test).round()\n",
    "\n",
    "# Calculate and print overall accuracy\n",
    "overall_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Overall accuracy: {:2.3}\\n\".format(overall_accuracy))\n",
    "\n",
    "# Calculate and print accuracy for proton class\n",
    "proton_accuracy = accuracy_score(y_test[y_test==0], y_pred[y_test==0])\n",
    "print(\" - Proton accuracy: {:2.3}\".format(proton_accuracy))\n",
    "\n",
    "# Calculate and print accuracy for muon class\n",
    "muon_accuracy = accuracy_score(y_test[y_test==1], y_pred[y_test==1])\n",
    "print(\" - Muon accuracy: {:2.3}\\n\".format(muon_accuracy))\n",
    "\n",
    "# Calculate and plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "print_conf(conf_matrix, ['protons', 'muons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFUFjUW36o4g"
   },
   "source": [
    "Nice! We're getting almost perfect separation using only two input parameters!\n",
    "\n",
    "Is there any room for improving the current results? A more robust but straightforward way of making the input data interpretable for the algorithm is to keep the information of only a few nodes of each track. Our preprocessing is illustrated in the following figure (there are many combinations, we are showing just one practical example here):\n",
    "\n",
    "<div>\n",
    "<img src=\"https://cernbox.cern.ch/remote.php/dav/public-files/dmklFFGAgXGLNbL/reg.png?scalingup=0&preview=1&a=1&c=284811005584736256%3A78b96dfb&x=1920&y=1920\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "where we keep the dE/dx of the first 3 and last 5 nodes of each track, along with their 4 global parameters, building up an array of size 12. In cases where the track has less than 8 nodes, we fill the empty positions of the array with -1s. With this preprocessing, the input dataset **X** will consist of 59,578 vectors of size 12 each, resulting in a 59,578x12 matrix. The values to estimate, **y**, are the labels of each event: proton or muon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4NZa2hZ6o4g",
    "outputId": "92642595-e99e-46f6-f3b7-e3b4e74bab3f"
   },
   "outputs": [],
   "source": [
    "# Initialize the input and label arrays\n",
    "X = np.full((len(df), 12), -1, dtype=np.float32)  # 2D array of size (n_event, 12) filled with -1s\n",
    "y = np.zeros(len(df), dtype=np.float32)  # 1D array of size (n_event,)\n",
    "\n",
    "# Fill in the input and label arrays\n",
    "for event_n, event in df.iterrows():\n",
    "    # Extract the dE/dx values\n",
    "    NodeOrder = event['NodeOrder']\n",
    "    Nodededx = event['Nodededx'][NodeOrder]\n",
    "\n",
    "    # Get up to the first 3 nodes\n",
    "    nfirstnodes = min(Nodededx.shape[0], 3)\n",
    "    X[event_n, :nfirstnodes] = Nodededx[:nfirstnodes]\n",
    "\n",
    "    # Get up to the last 5 nodes\n",
    "    if Nodededx.shape[0] > nfirstnodes:\n",
    "        nlastnodes = min(Nodededx.shape[0] - 3, 5)\n",
    "        X[event_n, nfirstnodes:nfirstnodes + nlastnodes] = Nodededx[-nlastnodes:]\n",
    "\n",
    "    # Add the global parameters\n",
    "    X[event_n, -4] = event['TrkLen']\n",
    "    X[event_n, -3] = event['TrkEDepo']\n",
    "    X[event_n, -2] = event['TrkDir1']\n",
    "    X[event_n, -1] = event['TrkDir2']\n",
    "\n",
    "    # Extract the PID label and add it to the label array\n",
    "    pid_label = event['TruePID']\n",
    "    if pid_label == 2212:\n",
    "        y[event_n] = 0  # protons\n",
    "    elif pid_label == 13:\n",
    "        y[event_n] = 1  # muons\n",
    "    else:\n",
    "        y[event_n] = 2  # pions\n",
    "\n",
    "# Standardize the input dataset (mean=0, std=1)\n",
    "X_stan = scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qH2A2LAzCsV"
   },
   "source": [
    "To gain insights into the training data, it's a good practice to visualize it first. One way of doing this is by creating a histogram plot for each of the 12 features in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "bcFeWh3iP7E-",
    "outputId": "11bf124d-bac1-4ba9-951e-2e2312a50453"
   },
   "outputs": [],
   "source": [
    "param_names = ['dE/dx node 1', 'dE/dx node 2', 'dE/dx node 3', 'dE/dx node n-4',\\\n",
    "               'dE/dx node n-3', 'dE/dx node n-2', 'dE/dx node n-1', 'dE/dx node n', 'TrkLen',\\\n",
    "               'TrkEDepo', 'TrkDir1', 'TrkDir2']\n",
    "class_names = [\"proton\", \"muon\", \"pion\"]\n",
    "\n",
    "# Plot histogram for each feature\n",
    "plot_parameters(X, y, param_names, class_names, mode=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YePGg59gzOwH"
   },
   "source": [
    "To evaluate the performance of our neural network model, we need to split the preprocessed dataset into two parts again: one for training and one for testing. We will use the `train_test_split()` function from the sklearn library to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EIkyVK-vwLR"
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stan[y!=2], y[y!=2], test_size=0.4, \n",
    "                                                    random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hecj03Q5zXTi"
   },
   "source": [
    "To accommodate the new dataset, we need to modify the input layer of our network. Once that's done, we can define and train a new model on the modified dataset. Finally, we can evaluate the performance of the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Z30XFUvRAJC",
    "outputId": "74a61f62-ee0c-466d-bf60-0567232cbc3b"
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "num_features = 12 # TrkLen, TrkEDepo\n",
    "num_classes = 1 # One output unit is enough since it's a binary classification problem\n",
    "\n",
    "# Fully connected neural network model\n",
    "input_layer = Input(shape=(num_features,)) # Input layer\n",
    "hidden_layer_1 = Dense(10, activation='relu')(input_layer) # Hidden layer 1\n",
    "hidden_layer_2 = Dense(10, activation='relu')(hidden_layer_1) # Hidden layer 2\n",
    "output_layer = Dense(num_classes, activation='sigmoid')(hidden_layer_2) # Output layer\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test).round()\n",
    "print(\"Overall accuracy: {:.3f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\" - Proton accuracy: {:.3f}\".format(accuracy_score(y_test[y_test==0], y_pred[y_test==0])))\n",
    "print(\" - Muon accuracy: {:.3f}\\n\".format(accuracy_score(y_test[y_test==1], y_pred[y_test==1])))\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "print_conf(conf_matrix, ['protons', 'muons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjXwKJZAzz9R"
   },
   "source": [
    "The results are impressive for a binary classification problem. However, it's important to note that our dataset has a third type of particle (pions) that we have ignored, which could be a significant drawback for some applications. Nevertheless, our neural network architecture can be extended to handle multi-class classification problems (where k>2) by modifying the output layer (number of neurons and activation function) and loss function appropriately (**categorical cross-entropy**, which is a generalisation of the binary cross-entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRlHNKF_TG_g",
    "outputId": "1fcf11ef-6a02-4238-a05f-20b04e9fb5dd"
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "num_features = 12 # TrkLen, TrkEDepo\n",
    "num_classes = 3 # Three outputs are needed (multi-class classification problem): proton, muon, and pion\n",
    "\n",
    "# Fully connected neural network model\n",
    "input_layer = Input(shape=(num_features,)) # Input layer\n",
    "hidden_layer_1 = Dense(10, activation='relu')(input_layer) # Hidden layer 1\n",
    "hidden_layer_2 = Dense(10, activation='relu')(hidden_layer_1) # Hidden layer 2\n",
    "output_layer = Dense(num_classes, activation='sigmoid')(hidden_layer_2) # Output layer\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stan, y, test_size=0.4,\n",
    "                                                    random_state=7)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "print(\"Overall accuracy: {:2.3}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\" - Proton accuracy: {:2.3}\".format(accuracy_score(y_test[y_test==0], y_pred[y_test==0])))\n",
    "print(\" - Muon accuracy: {:2.3}\".format(accuracy_score(y_test[y_test==1], y_pred[y_test==1])))\n",
    "print(\" - Pion accuracy: {:2.3}\\n\".format(accuracy_score(y_test[y_test==2], y_pred[y_test==2])))\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "print_conf(conf_matrix, ['protons', 'muons', 'pions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ek-qpcnGmR2N"
   },
   "source": [
    "To increase the capacity of our model and make it more capable of learning complex patterns, we can add more layers and neurons per layer. However, we need to be careful not to overfit the training data and generalize well to unseen data. Regularization techniques, such as dropout and weight decay, can help prevent overfitting. Moreover, it's essential to monitor the performance of the model on a validation set during training to ensure that we are not overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0slZOIEmlor",
    "outputId": "149fc82d-641a-49cf-c4c4-80ab87eb437c"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Define the number of features and classes\n",
    "num_features = 12 # TrkLen, TrkEDepo\n",
    "num_classes = 3 # proton, muon, and pion\n",
    "\n",
    "# Fully connected neural network model\n",
    "input_layer = Input(shape=(num_features,)) # Input layer\n",
    "hidden_layer_1 = Dense(100, activation='relu')(input_layer) # Hidden layer 1\n",
    "hidden_layer_2 = Dense(100, activation='relu')(hidden_layer_1) # Hidden layer 2\n",
    "hidden_layer_3 = Dense(100, activation='relu')(hidden_layer_2) # Hidden layer 3\n",
    "hidden_layer_4 = Dense(100, activation='relu')(hidden_layer_3) # Hidden layer 4\n",
    "output_layer = Dense(num_classes, activation='softmax')(hidden_layer_4) # Output layer\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss: {:.3f}\".format(loss))\n",
    "print(\"Test accuracy: {:.3f}\".format(accuracy))\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "# Print accuracy for each class and confusion matrix\n",
    "print(\"Accuracy for each class:\")\n",
    "print(\"Proton accuracy: {:.3f}\".format(accuracy_score(y_test[y_test==0], y_pred[y_test==0])))\n",
    "print(\"Muon accuracy: {:.3f}\".format(accuracy_score(y_test[y_test==1], y_pred[y_test==1])))\n",
    "print(\"Pion accuracy: {:.3f}\".format(accuracy_score(y_test[y_test==2], y_pred[y_test==2])))\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print_conf(conf_matrix, ['protons', 'muons', 'pions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McWnT5jTysUa"
   },
   "source": [
    "## Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVOpKD9eDg78"
   },
   "source": [
    "[Convolutional neural networks (CNNs)](https://direct.mit.edu/neco/article-abstract/1/4/541/5515/Backpropagation-Applied-to-Handwritten-Zip-Code?redirectedFrom=fulltext) have been highly successful in a variety of [HEP tasks](https://iml-wg.github.io/HEPML-LivingReview/), particularly in image classification problems. A CNN is a type of neural network that applies a series of filters, using convolutions, followed by spatial pooling, in sequence to extract increasingly powerful and abstract features from the input image [[citation](http://dl.acm.org/citation.cfm?id=2999134.2999257)].\n",
    "\n",
    "Each filter in a CNN consists of a set of values that are learned by the network during the training process. CNNs are typically deep neural networks that consist of many convolutional layers, with the output from one convolutional layer forming the input to the next. The last layers of a CNN are usually fully connected layers, where the output layer is followed by a sigmoid or softmax activation function.\n",
    "\n",
    "To apply CNNs to our dataset, we can convert our 3D events into 2D images. One simple approach is to save the YZ projection of each event as a 2D image. We chose this projection to retain the Z-axis, which corresponds to the beam direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lm_EAPlaUhTx"
   },
   "outputs": [],
   "source": [
    "# Set up empty arrays to hold the data\n",
    "X = np.zeros(shape=(len(df), 58, 189, 1), dtype=np.float32)\n",
    "y = np.zeros(shape=(len(df),), dtype=np.float32)\n",
    "\n",
    "# Iterate over the events in the dataset and fill the arrays\n",
    "for event_n, event in df.iterrows():\n",
    "    # Get the position and energy deposition information for the event\n",
    "    NodePosY = event['NodePosY']\n",
    "    NodePosZ = event['NodePosZ']\n",
    "    Nodededx = event['Nodededx']\n",
    "\n",
    "    old_y, old_z, dedxs = -1, -1, []\n",
    "    # Loop over each voxel in the event and calculate the average energy deposition\n",
    "    # in each voxel\n",
    "    for index in range(len(NodePosY)):\n",
    "        y_coord, z_coord = NodePosY[index], NodePosZ[index]\n",
    "        y_coord, z_coord = map_value(y_coord, z_coord) # Convert coords into voxels\n",
    "\n",
    "        # If we are still in the same voxel, add the current energy deposition to the list\n",
    "        if index == 0 or (y_coord == old_y and z_coord == old_z):\n",
    "            dedxs.append(Nodededx[index])\n",
    "        # If we have moved to a new voxel, calculate the average energy deposition in the\n",
    "        # previous voxel and add it to the X array\n",
    "        else:\n",
    "            X[event_n, old_y, old_z, 0] = np.mean(dedxs)\n",
    "            old_y, old_z, dedxs = y_coord, z_coord, [Nodededx[index]]\n",
    "\n",
    "    # Calculate the average energy deposition in the last voxel and add it to the X array\n",
    "    X[event_n, old_y, old_z, 0] = np.mean(dedxs)\n",
    "\n",
    "    # Set the label for the event\n",
    "    pid_label = event['TruePID']\n",
    "    if pid_label == 2212:\n",
    "        pid_label = 0 # protons\n",
    "    elif pid_label == 13: \n",
    "        pid_label = 1 # muons\n",
    "    else:\n",
    "        pid_label = 2 # pions\n",
    "    y[event_n] = pid_label\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n",
    "                                                    random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnOBphBcjy1D"
   },
   "source": [
    "To visualize the results of the code, we can plot two different views of the same 3D event, along with the corresponding YZ projection. This helps us ensure that the generated 2D images capture the important features of the original 3D events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "1k7W_ne8fvWJ",
    "outputId": "958f1aa9-047e-4bab-9a35-078b5609998e"
   },
   "outputs": [],
   "source": [
    "# Set the event number to plot\n",
    "event_number = 0\n",
    "\n",
    "# Call the plot_projection function to visualise the event:\n",
    "plot_projection(df, event_number, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EcFELvFxj0a"
   },
   "source": [
    "We will implement a convolutional neural network (CNN) that consists of several layers to classify the particle type of each event based on its YZ projection. The network will take a YZ projection of an event as input, which will be fed into several convolutional layers followed by max pooling. After that, the output will be flattened and fed into a few fully connected layers. Finally, the output layer will use the softmax activation function to output the predicted probability distribution over the three particle types: protons, muons, and pions:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://cernbox.cern.ch/remote.php/dav/public-files/lJYfLWXBsUaYWnO/cnn.png?scalingup=0&preview=1&a=1&c=284811005316300800%3A0fa0ba41&x=1920&y=1920\" width=\"900\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QApgnODzX5EQ",
    "outputId": "870a18a0-3156-4383-ba0d-4ebbf6a2db57"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Define the input shape of the CNN (input image shape)\n",
    "inp_shape = (58, 189, 1)\n",
    "\n",
    "# create the input layer\n",
    "input_layer = Input(shape=inp_shape)\n",
    "\n",
    "# add the first convolutional layer with 16 filters, filter size of (6, 18), and stride of (2, 3), followed by ReLU activation\n",
    "x = Conv2D(16, (6,18), padding='valid', strides=(2,3), activation='relu')(input_layer)\n",
    "\n",
    "# add the first max pooling layer with pool size of (2, 3) and stride of (2, 3)\n",
    "x = MaxPooling2D(pool_size=(2,3), strides=(2,3))(x)\n",
    "\n",
    "# add the second convolutional layer with 32 filters, filter size of (3, 3), and stride of (2, 3), followed by ReLU activation\n",
    "x = Conv2D(32, (3,3), padding='valid', strides=(2,3), activation='relu')(x)\n",
    "\n",
    "# add the second max pooling layer with pool size of (2, 3) and stride of (2, 3)\n",
    "x = MaxPooling2D(pool_size=(2,3), strides=(2,3))(x)\n",
    "\n",
    "# flatten the 3D output to 1D\n",
    "x = Flatten()(x)\n",
    "\n",
    "# add a fully connected layer with 64 units and ReLU activation\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# add the output layer with 3 units and softmax activation for multiclass classification\n",
    "output = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# create the model with the input and output layers\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# compile the model with sparse categorical cross-entropy loss, Adam optimizer, and accuracy as the metric\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's great to see the CNN coming together! Now let's train the model and see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHuTPTVWladz",
    "outputId": "9c8e97a8-ade3-4681-cac2-76fb18a094b1"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, verbose=1)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "print(\"Overall accuracy: {:2.3}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\" - Proton accuracy: {:2.3}\".format(accuracy_score(y_test[y_test==0], y_pred[y_test==0])))\n",
    "print(\" - Muon accuracy: {:2.3}\".format(accuracy_score(y_test[y_test==1], y_pred[y_test==1])))\n",
    "print(\" - Pion accuracy: {:2.3}\\n\".format(accuracy_score(y_test[y_test==2], y_pred[y_test==2])))\n",
    "conf_matrix = confusion_matrix(y_pred, y_test)\n",
    "print_conf(conf_matrix, ['protons', 'muons', 'pions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VEiG_FQk2YM"
   },
   "source": [
    "It's important to note that the CNN was trained on a much more limited piece of information compared to the fully-connected network (FCN). While the FCN was trained on many well understood physics informations, such as the track length, direction, and energy deposited, the CNN was only trained on 2D signatures of the tracks. However, this doesn't necessarily mean that the CNN is worse than the FCN. It's important for scientists to understand which method is best for each situation, and in this case, the CNN was able to achieve a respectable accuracy given its limited input information. In fact, by adding capacity to the CNN by designing wider and deeper networks, it's possible to improve upon the results achieved by the FCN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1vfmgtEIKdw"
   },
   "source": [
    "##Extra work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2BF4oRRITXC"
   },
   "source": [
    "To improve the performance of the models, one can increase their capacity by designing wider networks (adding more neurons or convolutional filters per layer) and deeper networks (adding more layers). Experimenting with different architectures and hyperparameters is crucial for achieving better results.\n",
    "\n",
    "Useful links:\n",
    "\n",
    "- How to Control Neural Network Model Capacity With Nodes and Layers: https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/.\n",
    "- TensorFlow 2 quickstart for beginners: https://www.tensorflow.org/tutorials/quickstart/beginner.\n",
    "- Building a Convolutional Neural Network Using TensorFlow â€“ Keras: https://www.analyticsvidhya.com/blog/2021/06/building-a-convolutional-neural-network-using-tensorflow-keras/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DX25P_WIovR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of neural_nets.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
